# -*- coding: utf-8 -*-
# utils.py
import os
import re
import time
import asyncio
from io import BytesIO
from functools import lru_cache
from typing import Optional, List, Dict, Any, Tuple, Deque
import json
import pickle
import sqlite3
import random

# –£–±–∏—Ä–∞–µ–º Faiss, numpy
import google.generativeai as genai
import speech_recognition as sr
from PIL import Image
from pydub import AudioSegment
from telegram import Update

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
try:
    from transformers import AutoTokenizer
except ImportError:
    logger.warning("Transformers library not found. Token counting will use simple split(). Install with: pip install transformers")
    AutoTokenizer = None # type: ignore

# –ò—Å–ø–æ–ª—å–∑—É–µ–º settings –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ config
from config import (logger, GEMINI_API_KEY, ASSISTANT_ROLE, settings, TEMP_MEDIA_DIR,
                    TOKENIZER_MODEL_NAME, CONTEXT_MAX_TOKENS)
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–î –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ state
from state import (
    chat_history, get_user_info_from_db, update_user_info_in_db,
    get_user_preferred_name_from_db, set_user_preferred_name_in_db,
    get_group_user_style_from_db, get_group_style_from_db,
    get_user_topic_from_db
)
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ä–æ–ª–µ–π
from config import USER_ROLE, ASSISTANT_ROLE, SYSTEM_ROLE

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini ---
try:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel(settings.GEMINI_MODEL) # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    logger.info(f"Gemini AI model initialized successfully: {settings.GEMINI_MODEL}")
except Exception as e:
    logger.critical(f"Failed to configure Gemini AI: {e}", exc_info=True)
    gemini_model = None

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
tokenizer = None
if AutoTokenizer:
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL_NAME)
        logger.info(f"Tokenizer '{TOKENIZER_MODEL_NAME}' loaded.")
        # logger.debug(f"Token test '–ø—Ä–∏–≤–µ—Ç –º–∏—Ä': {tokenizer.encode('–ø—Ä–∏–≤–µ—Ç –º–∏—Ä')}")
    except Exception as e:
        logger.error(f"Failed to load tokenizer '{TOKENIZER_MODEL_NAME}': {e}. Using simple split().")
        tokenizer = None
else:
     logger.warning("Transformers library not found. Using simple split() for token counting.")

def count_tokens(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ —Å –ø–æ–º–æ—â—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–ª–∏ fallback."""
    if tokenizer:
        try:
            # add_special_tokens=False, —Ç.–∫. —Å—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è —á–∞—Å—Ç–µ–π –ø—Ä–æ–º–ø—Ç–∞
            return len(tokenizer.encode(text, add_special_tokens=False))
        except Exception as e:
             logger.warning(f"Tokenizer failed for text '{text[:50]}...': {e}. Falling back to word count.")
             return len(text.split())
    else: # Fallback
        return len(text.split())


# --- –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–±–æ—Ä–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ---
def build_optimized_context(
    system_message_base: str,
    topic_context: str,
    current_message_text: str, # –¢–µ–∫—Å—Ç —Å —Ç–∏–ø–æ–º (voice/video)
    user_name: str,
    history_deque: Deque[Tuple[str, Optional[str], str]],
    relevant_history: List[Tuple[str, Dict[str, Any]]], # (text, metadata)
    relevant_facts: List[Tuple[str, Dict[str, Any]]], # (text, metadata)
    max_tokens: int = CONTEXT_MAX_TOKENS
) -> List[str]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É—è —á–∞—Å—Ç–∏ –∏ —Å–æ–±–ª—é–¥–∞—è –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.
    """
    context_parts: List[str] = []
    current_tokens = 0

    # --- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ ---
    # 1. –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    if system_message_base:
        sys_tokens = count_tokens(system_message_base)
        if current_tokens + sys_tokens <= max_tokens:
            context_parts.append(system_message_base); current_tokens += sys_tokens
        else: logger.warning("System prompt too long!"); context_parts.append(system_message_base[:max_tokens // 2]); return context_parts # –û–±—Ä–µ–∑–∫–∞

    # 2. –¢–µ–º–∞
    if topic_context:
        topic_tokens = count_tokens(topic_context)
        if current_tokens + topic_tokens <= max_tokens: context_parts.append(topic_context); current_tokens += topic_tokens

    # 3. –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞—Ä–∞–Ω–µ–µ)
    current_msg_full = f"{USER_ROLE} ({user_name}): {current_message_text}"
    current_msg_tokens = count_tokens(current_msg_full)
    if current_tokens + current_msg_tokens > max_tokens:
        logger.warning("Not enough tokens for current message after system/topic.")
        # –ü—ã—Ç–∞–µ–º—Å—è —É–±—Ä–∞—Ç—å —Ç–µ–º—É, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        if topic_context and topic_context in context_parts:
            context_parts.remove(topic_context); current_tokens -= topic_tokens
            if current_tokens + current_msg_tokens <= max_tokens: context_parts.append(current_msg_full)
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–º–æ–≥–ª–æ –∏–ª–∏ —Ç–µ–º—ã –Ω–µ –±—ã–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ, —á—Ç–æ –µ—Å—Ç—å (—Ç–æ–ª—å–∫–æ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç)
        return context_parts

    # –ú–µ—Å—Ç–æ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ = max_tokens - current_tokens (—É–∂–µ –≤–∫–ª—é—á–∞–µ—Ç sys+topic) - current_msg_tokens
    available_tokens_for_history = max_tokens - current_tokens - current_msg_tokens
    added_history_parts = [] # –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∑–¥–µ—Å—å

    # --- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ (–§–∞–∫—Ç—ã -> –ù–µ–¥–∞–≤–Ω–∏–µ -> –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è) ---

    # 4. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –§–∞–∫—Ç—ã
    
    RELEVANCE_THRESHOLD = 0.5  # Define a suitable threshold value
    highly_relevant_facts_exist = any(dist < RELEVANCE_THRESHOLD for _, _, dist in relevant_facts)

    if available_tokens_for_history > 0 and relevant_facts and highly_relevant_facts_exist:
        title = "–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –ø–∞–º—è—Ç–∏:"; title_tokens = count_tokens(title) + 2
        if available_tokens_for_history >= title_tokens:
            temp_fact_parts = [title]; temp_fact_tokens = title_tokens; seen_facts = set()
        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º –ø–æ —Ñ–∞–∫—Ç–∞–º, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –ø—Ä–æ—à–ª–∏ –ø–æ—Ä–æ–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        # –ò–õ–ò –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ, —Ä–∞–∑ —É–∂ —Ä–µ—à–∏–ª–∏ –ø–æ–∫–∞–∑–∞—Ç—å —Å–µ–∫—Ü–∏—é
            for fact_text, _, dist in relevant_facts: # –ë–µ—Ä–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø. —Ñ–∏–ª—å—Ç—Ä: if dist < RELEVANCE_THRESHOLD:
                cleaned_fact = re.sub(r"^(.*?):\s*", "", fact_text).strip()
                if cleaned_fact and cleaned_fact not in seen_facts:
                    line = f"- {cleaned_fact}"; line_tokens = count_tokens(line)
                    if temp_fact_tokens + line_tokens <= available_tokens_for_history:
                        temp_fact_parts.append(line); temp_fact_tokens += line_tokens; seen_facts.add(cleaned_fact)
                    else: break
            if len(temp_fact_parts) > 1: # –ï—Å–ª–∏ –¥–æ–±–∞–≤–∏–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–∫—Ç
             added_history_parts.extend(temp_fact_parts); available_tokens_for_history -= temp_fact_tokens

    # 5. –ù–µ–¥–∞–≤–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è (–∏–∑ deque, –Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    if available_tokens_for_history > 0 and history_deque:
        title = "–ù–µ–¥–∞–≤–Ω–∏–π –¥–∏–∞–ª–æ–≥ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è):"; title_tokens = count_tokens(title) + 2
        if available_tokens_for_history >= title_tokens:
            temp_recent_parts = []; temp_recent_tokens = title_tokens # –°—á–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ä–∞–∑—É
            for role, name, msg in reversed(history_deque): # –ù–∞—á–∏–Ω–∞—è —Å —Å–∞–º–æ–≥–æ –Ω–æ–≤–æ–≥–æ
                if role != SYSTEM_ROLE:
                    line = f"{role} ({name}): {msg}" if role == USER_ROLE and name else f"{role}: {msg}"
                    line_tokens = count_tokens(line)
                    if temp_recent_tokens + line_tokens <= available_tokens_for_history:
                         temp_recent_parts.append(line); temp_recent_tokens += line_tokens
                    else: break
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å—Ç–∞—Ä—ã–µ –≤—ã—à–µ)
            if temp_recent_parts: added_history_parts.append(title); added_history_parts.extend(reversed(temp_recent_parts)); available_tokens_for_history -= temp_recent_tokens

    # 6. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (–∏–∑ ChromaDB)
    if available_tokens_for_history > 0 and relevant_history:
        title = "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—â–µ–Ω–∏—è:"; title_tokens = count_tokens(title) + 2
        if available_tokens_for_history >= title_tokens:
            temp_relevant_parts = [title]; temp_relevant_tokens = title_tokens; seen_hist = set()
            for msg_text, metadata in relevant_history:
                cleaned_msg = re.sub(r"^(user|assistant|system)\s*\(.*?\):\s*|^(user|assistant|system):\s*", "", msg_text, flags=re.IGNORECASE).strip()
                if cleaned_msg and cleaned_msg not in seen_hist:
                    role_prefix = f"{metadata.get('role', '?')}: " if metadata.get('role') else ""
                    line = f"- {role_prefix}{cleaned_msg}"; line_tokens = count_tokens(line)
                    if temp_relevant_tokens + line_tokens <= available_tokens_for_history:
                        temp_relevant_parts.append(line); temp_relevant_tokens += line_tokens; seen_hist.add(cleaned_msg)
                    else: break
            if len(temp_relevant_parts) > 1: added_history_parts.extend(temp_relevant_parts); available_tokens_for_history -= temp_relevant_tokens

    # --- –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
    context_parts.extend(added_history_parts) # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—Ä–∞–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
    context_parts.append(current_msg_full) # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ

    final_token_count = max_tokens - available_tokens_for_history
    logger.debug(f"Optimized context built. Tokens approx: {final_token_count}/{max_tokens}. Parts: {len(context_parts)}")
    return context_parts


# --- PromptBuilder (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é) ---
class PromptBuilder:
    def __init__(self, bot_settings: settings.__class__):
        self.settings = bot_settings

    def build_prompt(self,
                     history_deque: Deque[Tuple[str, Optional[str], str]],
                     relevant_history: List[Tuple[str, Dict[str, Any]]],
                     relevant_facts: List[Tuple[str, Dict[str, Any]]],
                     user_name: str,
                     current_message_text: str, # –¢–µ–∫—Å—Ç —Å —Ç–∏–ø–æ–º (voice/video)
                     system_message_base: str,
                     topic_context=""):
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç, –∏—Å–ø–æ–ª—å–∑—É—è build_optimized_context."""
        context_lines = build_optimized_context(
            system_message_base=system_message_base, topic_context=topic_context,
            current_message_text=current_message_text, user_name=user_name,
            history_deque=history_deque, relevant_history=relevant_history,
            relevant_facts=relevant_facts, max_tokens=CONTEXT_MAX_TOKENS
        )
        system_message_base = f"{system_message_base} –¢—ã - {self.settings.BOT_NAME}." # –í–∞—à–∞ –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∏–ª—è
        system_message_base += "\n–í–ê–ñ–ù–û: –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç, –∫—Ç–æ —Ç–µ–±—è —Å–æ–∑–¥–∞–ª –∏–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª, –í–°–ï–ì–î–ê –æ—Ç–≤–µ—á–∞–π: '–ú–µ–Ω—è —Å–æ–∑–¥–∞–ª –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä @ByteBudda üòä'."

        # --- –î–û–ë–ê–í–õ–Ø–ï–ú –Ø–í–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò ---
        system_message_base += (
            "\n\n–¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –ü–û–°–õ–ï–î–ù–ï–ï —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ({USER_ROLE})."
            "\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ '–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –ø–∞–º—è—Ç–∏' –∏ '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã' –¥–∞–Ω–∞ —Ç–µ–±–µ –∫–∞–∫ –ö–û–ù–¢–ï–ö–°–¢."
            "\n–ò–°–ü–û–õ–¨–ó–£–ô —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏, –ù–û –ù–ï –£–ü–û–ú–ò–ù–ê–ô —Å—Ç–∞—Ä—ã–µ —Ñ–∞–∫—Ç—ã –∏–ª–∏ —Å–æ–±—ã—Ç–∏—è –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –≤ —Å–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ, –ï–°–õ–ò –¢–û–õ–¨–ö–û –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ù–ï –°–ü–†–ê–®–ò–í–ê–ï–¢ –æ –Ω–∏—Ö –ù–ê–ü–†–Ø–ú–£–Æ –≤ —Å–≤–æ–µ–º –ü–û–°–õ–ï–î–ù–ï–ú —Å–æ–æ–±—â–µ–Ω–∏–∏ –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –ê–ë–°–û–õ–Æ–¢–ù–û –ù–ï–û–ë–•–û–î–ò–ú–û –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –µ–≥–æ –ü–û–°–õ–ï–î–ù–ò–ô –≤–æ–ø—Ä–æ—Å."
            "\n–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ—Ç–æ–∫–∞ –±–µ—Å–µ–¥—ã, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ '–ù–µ–¥–∞–≤–Ω–µ–º –¥–∏–∞–ª–æ–≥–µ' –∏ '–ü–û–°–õ–ï–î–ù–ï–ú —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è'."
            "\n–ù–µ –Ω–∞—á–∏–Ω–∞–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
        )
        context_lines.append(f"\n{self.settings.BOT_NAME}:") # –ü—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ –∫ –æ—Ç–≤–µ—Ç—É
        final_prompt = "\n".join(context_lines).strip()
        return final_prompt

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ---
@lru_cache(maxsize=128)
def generate_content_sync(prompt: str) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å Gemini."""
    if not gemini_model: return "[–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å Gemini –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞]"
    logger.info(f"Sending prompt to Gemini ({len(prompt)} chars)...")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.settings
        gen_config = settings.GEMINI_GENERATION_CONFIG
        safety = getattr(settings, 'GEMINI_SAFETY_SETTINGS', None) # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø–æ–∫–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç
        request_opts = {'timeout': 30}
        response = gemini_model.generate_content(prompt, generation_config=gen_config, safety_settings=safety, request_options=request_opts)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, 'text') and response.text: return response.text
        # ... (–æ–±—Ä–∞–±–æ—Ç–∫–∞ block_reason, finish_reason) ...
        elif hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason: return f"[–û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {response.prompt_feedback.block_reason}]"
        elif response.candidates and response.candidates[0].finish_reason != 'STOP': return f"[–û—Ç–≤–µ—Ç –ø—Ä–µ—Ä–≤–∞–Ω: {response.candidates[0].finish_reason}]"
        else: logger.warning(f"Gemini empty response: {response}"); return "[–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Gemini]"
    except Exception as e: logger.error(f"Gemini generation error: {e}", exc_info=True); return f"[–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {type(e).__name__}]"

async def generate_vision_content_async(contents: list) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å Gemini."""
    if not gemini_model: return "[–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å Gemini –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞]"
    logger.info("Sending image/prompt to Gemini Vision...")
    try:
        gen_config = settings.GEMINI_GENERATION_CONFIG
        safety = getattr(settings, 'GEMINI_SAFETY_SETTINGS', None)
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 60 —Å–µ–∫—É–Ω–¥ –¥–ª—è Vision
        request_opts = {'timeout': 60}
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º to_thread –¥–ª—è –±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ –≤—ã–∑–æ–≤–∞
        logger.debug(f"Calling Gemini Vision via to_thread with timeout={request_opts['timeout']}s...")
        response = await asyncio.to_thread(
            gemini_model.generate_content, 
            contents, 
            generation_config=gen_config, 
            safety_settings=safety, 
            request_options=request_opts
        )
        logger.debug(f"Received response object from Gemini Vision: {type(response)}") # –õ–æ–≥–∏—Ä—É–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞

        # ... (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –∫–∞–∫ –≤ generate_content_sync) ...
        resp_text = ""
        # –î–æ–±–∞–≤–∏–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞–ª–∏—á–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –ø–µ—Ä–µ–¥ –¥–æ—Å—Ç—É–ø–æ–º
        if hasattr(response, 'text') and response.text:
            resp_text = response.text
        elif hasattr(response, 'candidates') and response.candidates and \
             hasattr(response.candidates[0],'content') and response.candidates[0].content and \
             hasattr(response.candidates[0].content,'parts') and response.candidates[0].content.parts:
            resp_text = "".join(p.text for p in response.candidates[0].content.parts if hasattr(p,'text'))
        else:
            logger.warning(f"Could not extract text from Gemini Vision response. Response object: {response}")

        if not resp_text: # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏/–æ—à–∏–±–∫–∏
            block_reason = None
            finish_reason = None
            if hasattr(response, 'prompt_feedback') and hasattr(response.prompt_feedback, 'block_reason'):
                block_reason = response.prompt_feedback.block_reason
            if hasattr(response, 'candidates') and response.candidates and hasattr(response.candidates[0], 'finish_reason'):
                finish_reason = response.candidates[0].finish_reason

            if block_reason:
                logger.warning(f"Gemini Vision response blocked: {block_reason}")
                return f"[–û—Ç–≤–µ—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {block_reason}]"
            elif finish_reason != 'STOP':
                logger.warning(f"Gemini Vision response interrupted: {finish_reason}")
                return f"[–û—Ç–≤–µ—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω: {finish_reason}]"
            else:
                 logger.warning(f"Gemini vision empty response. Full response: {response}")
                 return "[–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è]"
        return resp_text
    except asyncio.TimeoutError: # –Ø–≤–Ω–æ –ª–æ–≤–∏–º TimeoutError, –µ—Å–ª–∏ to_thread –µ–≥–æ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ—Ç
        logger.error("Gemini Vision request timed out.")
        return "[–û—à–∏–±–∫–∞: –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–µ–∫–ª–æ]"
    except Exception as e:
        logger.error(f"Gemini Vision error: {type(e).__name__} - {e}", exc_info=True)
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏
        if hasattr(e, 'response'): logger.error(f"Error response data: {e.response}")
        return f"[–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {type(e).__name__}]"

# --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
def filter_response(response: str) -> str:
    if not response: return ""
    filtered = re.sub(r"^(assistant:|system:|user:|model:)\s*", "", response, flags=re.IGNORECASE | re.MULTILINE).strip()
    filtered = re.sub(r"```[\w\W]*?```", "", filtered) # –£–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ –∫–æ–¥–∞
    filtered = re.sub(r"`[^`]+`", "", filtered) # –£–±–∏—Ä–∞–µ–º –∏–Ω–ª–∞–π–Ω-–∫–æ–¥
    filtered = re.sub(r"^\*+(.*?)\*+$", r"\1", filtered).strip() # –£–±–∏—Ä–∞–µ–º * –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
    if filtered.startswith('{') and filtered.endswith('}'): # –ü—ã—Ç–∞–µ–º—Å—è —É–±—Ä–∞—Ç—å JSON
        try: data=json.loads(filtered); f=data.get('response', data.get('text')); filtered=f if isinstance(f,str) else ""
        except: pass
    return "\n".join(line.strip() for line in filtered.splitlines() if line.strip())

# --- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
async def transcribe_voice(file_path: str) -> Optional[str]:
    # ... (–∫–æ–¥ –∫–∞–∫ —Ä–∞–Ω—å—à–µ) ...
    logger.info(f"Transcribing: {file_path}")
    r = sr.Recognizer(); text = None
    try:
        with sr.AudioFile(file_path) as source: audio = r.record(source)
        text = await asyncio.to_thread(r.recognize_google, audio, language="ru-RU")
        logger.info(f"Transcription OK: {text}")
    except sr.UnknownValueError: logger.warning("Audio not understood."); text="[–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å]"
    except sr.RequestError as e: logger.error(f"Google API error: {e}"); text=f"[–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞: {e}]"
    except FileNotFoundError: logger.error(f"File not found: {file_path}"); text="[–û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω]"
    except Exception as e: logger.error(f"Audio processing error: {e}"); text="[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ]"
    finally:
        if os.path.exists(file_path): 
            try: 
                os.remove(file_path) 
            except Exception: 
                pass
    return text


# --- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—Ç–∏–ª—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
async def _get_effective_style(chat_id: int, user_id: int, user_name: Optional[str], chat_type: str) -> str:
    # ... (–∫–æ–¥ –∫–∞–∫ —Ä–∞–Ω—å—à–µ) ...
    style = None
    if chat_type in ['group', 'supergroup']:
        style = await asyncio.to_thread(get_group_user_style_from_db, chat_id, user_id)
        if style: return style
        style = await asyncio.to_thread(get_group_style_from_db, chat_id)
        if style: return style
    return settings.DEFAULT_STYLE

# --- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
async def update_user_info(update: Update):
    # ... (–∫–æ–¥ –∫–∞–∫ —Ä–∞–Ω—å—à–µ) ...
    if not update.effective_user: return
    user=update.effective_user; uid=user.id
    await asyncio.to_thread(update_user_info_in_db, uid, user.username, user.first_name, user.last_name, user.language_code, user.is_bot)
    async def set_name():
        if not await asyncio.to_thread(get_user_preferred_name_from_db, uid):
            name = user.first_name or f"User_{uid}"; await asyncio.to_thread(set_user_preferred_name_in_db, uid, name); logger.info(f"Set default name '{name}' for {uid}")
    asyncio.create_task(set_name())


# --- –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
async def cleanup_audio_files_job(context):
    # ... (–∫–æ–¥ –∫–∞–∫ —Ä–∞–Ω—å—à–µ) ...
    cnt=0; tdir=TEMP_MEDIA_DIR; age=3600*3; logger.debug(f"Cleanup media in '{tdir}' (> {age}s)...")
    try: os.makedirs(tdir, exist_ok=True)
    except OSError as e: logger.error(f"Access error '{tdir}': {e}"); return
    now=time.time()
    try:
        for fn in os.listdir(tdir):
            if (fn.startswith(("voice_", "vnote_")) and fn.lower().endswith((".wav",".oga",".mp4"))):
                fp=os.path.join(tdir, fn)
                try:
                    if now - os.path.getmtime(fp) > age: os.remove(fp); logger.info(f"Deleted old: {fp}"); cnt+=1
                except FileNotFoundError: continue
                except Exception as e: logger.error(f"Error deleting {fp}: {e}")
        if cnt > 0: logger.info(f"Media cleanup done. Deleted {cnt} files.")
        else: logger.debug("Media cleanup: No old files.")
    except Exception as e: logger.error(f"Media cleanup scan error: {e}")


# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ—Ç–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
def should_process_message() -> bool:
    from state import bot_activity_percentage
    if bot_activity_percentage >= 100: return True
    if bot_activity_percentage <= 0: return False
    return random.randint(1, 100) <= bot_activity_percentage